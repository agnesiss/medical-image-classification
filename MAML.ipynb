{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15e6bc8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MAML:\n",
    "    def __init__(self, model, inner_lr=0.01, outer_lr=0.001, inner_steps=1):\n",
    "        self.model = model.to(device)\n",
    "        self.meta_optimizer = torch.optim.Adam(self.model.parameters(), lr=outer_lr)\n",
    "        self.inner_lr = inner_lr\n",
    "        self.inner_steps = inner_steps\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def clone_model(self):\n",
    "        return copy.deepcopy(self.model)\n",
    "\n",
    "    def inner_loop(self, support_images, support_labels):\n",
    "        self.model.train()\n",
    "        params = dict(self.model.named_parameters())  # initial params\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     initial_preds = functional_call(self.model, params, support_images)\n",
    "        #     initial_loss = self.loss_fn(initial_preds, support_labels)\n",
    "        #     print(\"Initial support loss:\", initial_loss.item())\n",
    "\n",
    "        for _ in range(self.inner_steps):\n",
    "            preds = functional_call(self.model, params, support_images)\n",
    "            loss = self.loss_fn(preds, support_labels)\n",
    "            grads = torch.autograd.grad(loss, params.values(), create_graph=True)\n",
    "\n",
    "            # Differentiable update\n",
    "            params = {\n",
    "                name: param - self.inner_lr * grad\n",
    "                for (name, param), grad in zip(params.items(), grads)\n",
    "            }\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     updated_preds = functional_call(self.model, params, support_images)\n",
    "        #     updated_loss = self.loss_fn(updated_preds, support_labels)\n",
    "        #     print(\"Updated support loss:\", updated_loss.item())\n",
    "\n",
    "        return params\n",
    "\n",
    "    def outer_loop(self, tasks):\n",
    "        self.model.train()\n",
    "        meta_loss = 0.0\n",
    "\n",
    "        for support_set, query_set in tasks:\n",
    "            support_images, support_labels = support_set\n",
    "            query_images, query_labels = query_set\n",
    "\n",
    "            support_images = support_images.to(device)\n",
    "            support_labels = support_labels.to(device)\n",
    "            query_images = query_images.to(device)\n",
    "            query_labels = query_labels.to(device)\n",
    "\n",
    "            # Get adapted parameters\n",
    "            adapted_params = self.inner_loop(support_images, support_labels)\n",
    "\n",
    "            # Evaluate with adapted weights using functional_call\n",
    "            query_preds = functional_call(self.model, adapted_params, query_images)\n",
    "            loss = self.loss_fn(query_preds, query_labels)\n",
    "            meta_loss += loss\n",
    "\n",
    "        meta_loss /= len(tasks)\n",
    "        self.meta_optimizer.zero_grad()\n",
    "        meta_loss.backward()\n",
    "        self.meta_optimizer.step()\n",
    "\n",
    "        return meta_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5215ef13",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_binary_tasks(dataset, index_map, disease, k_shot=5, q_query=15, num_tasks=4):\n",
    "    tasks = []\n",
    "\n",
    "    pos_indices = index_map[disease][\"pos\"]\n",
    "    neg_indices = index_map[disease][\"neg\"]\n",
    "\n",
    "    for _ in range(num_tasks):\n",
    "        pos_idxs = random.sample(pos_indices, k_shot + q_query)\n",
    "        neg_idxs = random.sample(neg_indices, k_shot + q_query)\n",
    "\n",
    "        pos_samples = [dataset[i] for i in pos_idxs]\n",
    "        neg_samples = [dataset[i] for i in neg_idxs]\n",
    "\n",
    "        support = pos_samples[:k_shot] + neg_samples[:k_shot]\n",
    "        query = pos_samples[k_shot:] + neg_samples[k_shot:]\n",
    "\n",
    "        support_images = torch.stack([x for x, _ in support]).to(device)\n",
    "        support_labels = torch.stack([y for _, y in support]).to(device)\n",
    "        query_images = torch.stack([x for x, _ in query]).to(device)\n",
    "        query_labels = torch.stack([y for _, y in query]).to(device)\n",
    "\n",
    "        tasks.append(((support_images, support_labels), (query_images, query_labels)))\n",
    "\n",
    "    return tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49efc13",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def build_label_index_map(dataset, disease_names):\n",
    "    index_map = defaultdict(lambda: {\"pos\": [], \"neg\": []})\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        for disease_idx, disease in enumerate(disease_names):\n",
    "            if label[disease_idx] == 1:\n",
    "                index_map[disease][\"pos\"].append(i)\n",
    "            else:\n",
    "                index_map[disease][\"neg\"].append(i)\n",
    "\n",
    "    return index_map\n",
    "\n",
    "index_map = build_label_index_map(train_dataset, disease_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd057a59",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# training loop\n",
    "model = ResNet50_MultiLabel(15).to(device)\n",
    "maml = MAML(model, inner_lr=0.01, outer_lr=0.001, inner_steps=1)\n",
    "\n",
    "val_diseases = [\"Nodule\"]  # target disease\n",
    "val_tasks = []\n",
    "\n",
    "for disease in val_diseases:\n",
    "    tasks = get_binary_tasks(test_dataset, test_index_map, disease=disease, k_shot=5, q_query=15, num_tasks=4)\n",
    "    val_tasks.extend(tasks)\n",
    "\n",
    "start_time = time.time()\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for disease in disease_names:\n",
    "        if disease == \"Nodule\":  # leave out target\n",
    "            continue\n",
    "        tasks = get_binary_tasks(train_dataset, index_map, disease=disease, k_shot=5, q_query=15, num_tasks=8)\n",
    "        loss = maml.outer_loop(tasks)\n",
    "    if epoch % 10 == 0:\n",
    "\n",
    "        maml.model.eval()\n",
    "        val_losses = []\n",
    "        for (support_images, support_labels), (query_images, query_labels) in val_tasks:\n",
    "            adapted_params = maml.inner_loop(support_images.to(device), support_labels.to(device))\n",
    "            with torch.no_grad():\n",
    "                preds = functional_call(maml.model, adapted_params, query_images.to(device))\n",
    "                loss_fn = nn.BCEWithLogitsLoss()\n",
    "                val_loss = loss_fn(preds, query_labels.to(device))\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        elapsed = (time.time() - start_time) / 60\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {loss:.4f}, Val Loss = {avg_val_loss:.4f}, Time: {elapsed:.2f} min\")\n",
    "        maml.model.train()\n",
    "\n",
    "\n",
    "torch.save(maml.model.state_dict(), \"maml_resnet.pth\")\n",
    "print(\"âœ… MAML model saved as maml_resnet.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe992ad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_few_shot(maml, dataset, index_map, disease, disease_idx, k_shot=5, q_query=15, num_tasks=10):\n",
    "    pos_indices = index_map[disease][\"pos\"]\n",
    "    neg_indices = index_map[disease][\"neg\"]\n",
    "\n",
    "    accs = []\n",
    "    all_true = []\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "\n",
    "    for _ in range(num_tasks):\n",
    "        pos_batch_idxs = random.sample(pos_indices, k_shot + q_query)\n",
    "        neg_batch_idxs = random.sample(neg_indices, k_shot + q_query)\n",
    "\n",
    "        support_idxs = pos_batch_idxs[:k_shot] + neg_batch_idxs[:k_shot]\n",
    "        query_idxs = pos_batch_idxs[k_shot:] + neg_batch_idxs[k_shot:]\n",
    "\n",
    "        support_samples = [dataset[i] for i in support_idxs]\n",
    "        query_samples = [dataset[i] for i in query_idxs]\n",
    "\n",
    "        support_images = torch.stack([x for x, _ in support_samples]).to(device)\n",
    "        support_labels = torch.stack([y for _, y in support_samples]).to(device)\n",
    "        query_images = torch.stack([x for x, _ in query_samples]).to(device)\n",
    "        query_labels = torch.stack([y for _, y in query_samples]).to(device)\n",
    "\n",
    "        adapted_params = maml.inner_loop(support_images, support_labels)\n",
    "        preds = functional_call(maml.model, adapted_params, query_images)\n",
    "        probs = torch.sigmoid(preds[:, disease_idx])\n",
    "        preds_binary = (probs > 0.5).float()\n",
    "        true_binary = query_labels[:, disease_idx]\n",
    "\n",
    "        acc = (preds_binary == true_binary).sum().item() / len(preds_binary)\n",
    "        accs.append(acc)\n",
    "\n",
    "        all_true.extend(true_binary.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().detach().numpy())\n",
    "        all_preds.extend(preds_binary.cpu().numpy())\n",
    "\n",
    "    final_acc = sum(accs) / len(accs)\n",
    "    print(f\"[MAML] Few-shot Accuracy on '{disease}': {final_acc:.2f}\")\n",
    "\n",
    "    try:\n",
    "        auc = roc_auc_score(all_true, all_probs)\n",
    "        print(f\"[MAML] Few-shot AUC on '{disease}': {auc:.4f}\")\n",
    "    except ValueError:\n",
    "        print(f\"[MAML] Few-shot AUC on '{disease}': N/A (only one class present)\")\n",
    "\n",
    "    f1 = f1_score(all_true, all_preds, zero_division=0)\n",
    "    print(f\"[MAML] Few-shot F1 Score on '{disease}': {f1:.4f}\")\n",
    "\n",
    "    return final_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f5f972",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for target_disease in disease_names:\n",
    "    #target_disease = \"Nodule\"\n",
    "    target_disease_idx = disease_names.index(target_disease)\n",
    "\n",
    "    evaluate_few_shot(\n",
    "        maml,\n",
    "        test_dataset,\n",
    "        test_index_map,\n",
    "        disease=target_disease,\n",
    "        disease_idx=target_disease_idx,\n",
    "        k_shot=5,\n",
    "        q_query=15,\n",
    "        num_tasks=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4e8bd6",
   "metadata": {},
   "source": [
    "# Fine-tuning ResNet on small sample and compare with MAML\n",
    "Given that MAML is able to produce good result on samll samples, we want to compare it with the traditional approach of finetuning a model on samll samples. The experiement proceeds in the following steps\n",
    "\n",
    "Separte the samples into two set. (1) all other diseases except target disease \"Nodule\" (2) small samples of target diseases - 20 samples.\n",
    "\n",
    "Train the resnet model on dataset (1) as a pretrained model. And evaluate its performance\n",
    "Fine-tune the pretrained model on dataset (2) and evaluate again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95070f41",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "nodule_idx = disease_names.index(\"Nodule\")\n",
    "\n",
    "nodule_indices = [i for i, (_, label) in enumerate(filtered_dataset) if label[nodule_idx] == 1.0]\n",
    "non_nodule_indices = [i for i, (_, label) in enumerate(filtered_dataset) if label[nodule_idx] == 0.0]\n",
    "\n",
    "# Get small nodule dataset (20 train + 20 test)\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(nodule_indices)\n",
    "\n",
    "nodule_train_indices = nodule_indices[:20]\n",
    "nodule_test_indices = nodule_indices[20:40]\n",
    "\n",
    "pretrain_indices = non_nodule_indices\n",
    "\n",
    "nodule_train_dataset = Subset(filtered_dataset, nodule_train_indices)\n",
    "nodule_test_dataset = Subset(filtered_dataset, nodule_test_indices)\n",
    "pretrain_dataset = Subset(filtered_dataset, pretrain_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819c6098",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class BCEWithLogitsLossExceptNodule(nn.Module):\n",
    "    def __init__(self, nodule_index):\n",
    "        super().__init__()\n",
    "        self.nodule_index = nodule_index\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input_without_nodule = torch.cat([input[:, :self.nodule_index], input[:, self.nodule_index+1:]], dim=1)\n",
    "        target_without_nodule = torch.cat([target[:, :self.nodule_index], target[:, self.nodule_index+1:]], dim=1)\n",
    "        return self.bce(input_without_nodule, target_without_nodule)\n",
    "\n",
    "# Dataloader\n",
    "pretrain_loader = DataLoader(pretrain_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop (no Nodule)\n",
    "resnet_nodule = ResNet50_MultiLabel(15).to(device)\n",
    "loss_fn = BCEWithLogitsLossExceptNodule(nodule_idx)\n",
    "\n",
    "criterion = loss_fn\n",
    "optimizer = torch.optim.Adam(resnet_nodule.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5, 10, 15], gamma=0.5)\n",
    "\n",
    "start_time = time.time()\n",
    "train_loss_resnet_plot = []\n",
    "val_loss_resnet_plot = []\n",
    "\n",
    "\n",
    "for epoch in range(0, 5):\n",
    "    resnet_nodule.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in pretrain_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet_nodule(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(pretrain_loader)\n",
    "    train_loss_resnet_plot.append(avg_loss)\n",
    "    elapsed = (time.time() - start_time) / 60\n",
    "    print(f\"\\nEpoch {epoch}, Train Loss: {avg_loss:.4f}, Time: {elapsed:.2f} min\")\n",
    "\n",
    "\n",
    "torch.save(resnet_nodule.state_dict(), \"pretrained_resnet_no_nodule.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea594f2b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pre_resnet_nodule = ResNet50_MultiLabel(15).to(device)\n",
    "pre_resnet_nodule.load_state_dict(torch.load(\"pretrained_resnet_no_nodule.pth\"))\n",
    "evaluate_resnet(pre_resnet_nodule, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f4c84d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load pretrained weights\n",
    "resnet_nodule.load_state_dict(torch.load(\"pretrained_resnet_no_nodule.pth\"))\n",
    "\n",
    "# Modify the loss to focus on Nodule only\n",
    "class BCEWithLogitsLossForNodule(nn.Module):\n",
    "    def __init__(self, nodule_index):\n",
    "        super().__init__()\n",
    "        self.nodule_index = nodule_index\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input = input[:, self.nodule_index].unsqueeze(1)\n",
    "        target = target[:, self.nodule_index].unsqueeze(1)\n",
    "        return self.bce(input, target)\n",
    "\n",
    "\n",
    "nodule_loss_fn = BCEWithLogitsLossForNodule(nodule_idx)\n",
    "\n",
    "# Fine-tune\n",
    "nodule_train_loader = DataLoader(nodule_train_dataset, batch_size=4, shuffle=True, drop_last=True)\n",
    "\n",
    "criterion = nodule_loss_fn\n",
    "# freeze weights\n",
    "for name, param in resnet_nodule.backbone.named_parameters():\n",
    "    if not name.startswith('fc'):\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Only train classifier layer\n",
    "optimizer = torch.optim.Adam(resnet_nodule.backbone.fc.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5, 10, 15], gamma=0.5)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(20):\n",
    "    resnet_nodule.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in nodule_train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet_nodule(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        if outputs.requires_grad:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(nodule_train_loader)\n",
    "    elapsed = (time.time() - start_time) / 60\n",
    "    print(f\"\\n[Fine-tune] Epoch {epoch}, Train Loss: {avg_loss:.4f}, Time: {elapsed:.2f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a5308d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "nodule_test_loader = DataLoader(nodule_test_dataset, batch_size=4, shuffle=False)\n",
    "def evaluate_nodule(net, loader, nodule_idx):\n",
    "    net.eval()\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = torch.sigmoid(net(images))\n",
    "            all_probs.append(outputs[:, nodule_idx])\n",
    "            all_labels.append(labels[:, nodule_idx])\n",
    "\n",
    "    all_probs = torch.cat(all_probs).cpu()\n",
    "    all_labels = torch.cat(all_labels).cpu()\n",
    "\n",
    "    preds_binary = (all_probs > 0.5).float()\n",
    "\n",
    "    # Classification metrics\n",
    "    acc = (preds_binary == all_labels).sum().item() / len(all_labels)\n",
    "    precision = precision_score(all_labels, preds_binary, zero_division=0)\n",
    "    recall = recall_score(all_labels, preds_binary, zero_division=0)\n",
    "    f1 = f1_score(all_labels, preds_binary, zero_division=0)\n",
    "\n",
    "    print(f\"\\nðŸ“Š Evaluation on 'Nodule':\")\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "\n",
    "evaluate_nodule(resnet_nodule, nodule_test_loader, nodule_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dc50be",
   "metadata": {},
   "source": [
    "Finally, we evaluate the fine-tune model again. We can see the the AUCROC score for nodule improves from\n",
    "\n",
    "Nodule: AUC = 0.4754\n",
    "\n",
    "to\n",
    "\n",
    "Nodule: AUC = 0.5209\n",
    "\n",
    "Which shows the effect of fine-tuning. However, it is still significantly lower than the result of MAML which is 0.6630.\n",
    "\n",
    "This shows that traditional CNN network is better at learning given large number of samples, but perform badly if the samples is small, in comparison with MAML architecture."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
